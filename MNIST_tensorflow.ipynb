{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eladcohen333/MNIST/blob/main/MNIST_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlyfYkFLPWci"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VK16ZB7FyVUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load data\n"
      ],
      "metadata": {
        "id": "MeN10xsC6A1P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om8H30d6PZKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d51105a-669f-4b39-ab93-ee50c295dc39"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#normalzution"
      ],
      "metadata": {
        "id": "NCcUOyYA6F9m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWOiRz2JXLsL"
      },
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkGnwV6mXv-C"
      },
      "source": [
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSbuQssof8sT"
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (28,28,1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMX_TfLcfpE2"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=num_classes)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.regression logistic\n"
      ],
      "metadata": {
        "id": "ARcZnqPAadIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.python.keras.backend import shape\n",
        "model = keras.Sequential([\n",
        "                          keras.Input(shape=input_shape),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(num_classes, activation=tf.nn.softmax)\n",
        "\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "batch_size = 100\n",
        "epochs = 12\n",
        "metrics = ['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "ecRKZDOeabOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2eb628-2252-4039-cdff-38e17d49c828"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run model and results"
      ],
      "metadata": {
        "id": "XkpUmHU2eKQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_split = .1) \n",
        "print(\"*************************************\")\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_train  = model.evaluate(X_train, y_train)\n",
        "f1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\n",
        "f1_test = 2*(score[2]*score[3])/(score[2]+score[3])\n",
        "\n",
        "print(\"Summary Model \")\n",
        "print('train loss: {}'.format(score_train[0]))\n",
        "print('train acc: {}'.format(score_train[1]))\n",
        "print('train precision: {}'.format(score_train[2]))\n",
        "print('train recall: {}'.format(score_train[3]))\n",
        "print('train F1', f1_train)\n",
        "\n",
        "print(\"*************************************\")\n",
        "print('test loss: {}'.format(score[0]))\n",
        "print('test acc: {}'.format(score[1]))\n",
        "print('test precision: {}'.format(score[2]))\n",
        "print('test recall: {}'.format(score[3]))\n",
        "print('test F1', f1_test)\n",
        "\n",
        "print(\"*************************************\")\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "biases = model.layers[0].get_weights()[1]\n",
        "print('test weights ',weights)\n",
        "print('test biases ',biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-K9VJY6EabWe",
        "outputId": "52b8401f-a727-49aa-ec02-4c3ada899d11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "540/540 [==============================] - 4s 5ms/step - loss: 0.6591 - acc: 0.8348 - precision_3: 0.9487 - recall_3: 0.6695 - val_loss: 0.3271 - val_acc: 0.9177 - val_precision_3: 0.9583 - val_recall_3: 0.8728\n",
            "Epoch 2/12\n",
            "540/540 [==============================] - 3s 5ms/step - loss: 0.3604 - acc: 0.9020 - precision_3: 0.9400 - recall_3: 0.8626 - val_loss: 0.2724 - val_acc: 0.9258 - val_precision_3: 0.9525 - val_recall_3: 0.9018\n",
            "Epoch 3/12\n",
            "540/540 [==============================] - 2s 5ms/step - loss: 0.3204 - acc: 0.9117 - precision_3: 0.9395 - recall_3: 0.8842 - val_loss: 0.2540 - val_acc: 0.9290 - val_precision_3: 0.9507 - val_recall_3: 0.9090\n",
            "Epoch 4/12\n",
            "540/540 [==============================] - 3s 5ms/step - loss: 0.3017 - acc: 0.9160 - precision_3: 0.9395 - recall_3: 0.8935 - val_loss: 0.2434 - val_acc: 0.9353 - val_precision_3: 0.9539 - val_recall_3: 0.9167\n",
            "Epoch 5/12\n",
            "540/540 [==============================] - 2s 5ms/step - loss: 0.2905 - acc: 0.9193 - precision_3: 0.9402 - recall_3: 0.8985 - val_loss: 0.2373 - val_acc: 0.9358 - val_precision_3: 0.9529 - val_recall_3: 0.9180\n",
            "Epoch 6/12\n",
            "540/540 [==============================] - 3s 5ms/step - loss: 0.2831 - acc: 0.9216 - precision_3: 0.9412 - recall_3: 0.9024 - val_loss: 0.2332 - val_acc: 0.9367 - val_precision_3: 0.9531 - val_recall_3: 0.9212\n",
            "Epoch 7/12\n",
            "540/540 [==============================] - 2s 4ms/step - loss: 0.2771 - acc: 0.9231 - precision_3: 0.9411 - recall_3: 0.9057 - val_loss: 0.2301 - val_acc: 0.9368 - val_precision_3: 0.9522 - val_recall_3: 0.9222\n",
            "Epoch 8/12\n",
            "540/540 [==============================] - 2s 5ms/step - loss: 0.2727 - acc: 0.9237 - precision_3: 0.9413 - recall_3: 0.9079 - val_loss: 0.2280 - val_acc: 0.9392 - val_precision_3: 0.9534 - val_recall_3: 0.9238\n",
            "Epoch 9/12\n",
            "540/540 [==============================] - 2s 5ms/step - loss: 0.2692 - acc: 0.9246 - precision_3: 0.9426 - recall_3: 0.9094 - val_loss: 0.2266 - val_acc: 0.9397 - val_precision_3: 0.9550 - val_recall_3: 0.9257\n",
            "Epoch 10/12\n",
            "540/540 [==============================] - 3s 5ms/step - loss: 0.2660 - acc: 0.9257 - precision_3: 0.9426 - recall_3: 0.9107 - val_loss: 0.2255 - val_acc: 0.9397 - val_precision_3: 0.9552 - val_recall_3: 0.9270\n",
            "Epoch 11/12\n",
            "540/540 [==============================] - 2s 4ms/step - loss: 0.2635 - acc: 0.9264 - precision_3: 0.9431 - recall_3: 0.9120 - val_loss: 0.2246 - val_acc: 0.9407 - val_precision_3: 0.9555 - val_recall_3: 0.9260\n",
            "Epoch 12/12\n",
            "540/540 [==============================] - 2s 4ms/step - loss: 0.2610 - acc: 0.9272 - precision_3: 0.9439 - recall_3: 0.9137 - val_loss: 0.2238 - val_acc: 0.9392 - val_precision_3: 0.9533 - val_recall_3: 0.9278\n",
            "*************************************\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.2635 - acc: 0.9274 - precision_3: 0.9423 - recall_3: 0.9147\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2525 - acc: 0.9302 - precision_3: 0.9462 - recall_3: 0.9166\n",
            "Summary Model \n",
            "train loss: 0.25252705812454224\n",
            "train acc: 0.9302166700363159\n",
            "train precision: 0.9461696147918701\n",
            "train recall: 0.9166333079338074\n",
            "train F1 0.9311672998709893\n",
            "*************************************\n",
            "test loss: 0.2634541094303131\n",
            "test acc: 0.9273999929428101\n",
            "test precision: 0.9423096776008606\n",
            "test recall: 0.9146999716758728\n",
            "test F1 0.9282995764153537\n",
            "*************************************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-972288372721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.fit(X_train, y_train,\\n          batch_size = batch_size,\\n          epochs = epochs,\\n          validation_split = .1) \\nprint(\"*************************************\")\\nscore = model.evaluate(X_test, y_test)\\nscore_train  = model.evaluate(X_train, y_train)\\nf1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\\nf1_test = 2*(score[2]*score[3])/(score[2]+score[3])\\n\\nprint(\"Summary Model \")\\nprint(\\'train loss: {}\\'.format(score_train[0]))\\nprint(\\'train acc: {}\\'.format(score_train[1]))\\nprint(\\'train precision: {}\\'.format(score_train[2]))\\nprint(\\'train recall: {}\\'.format(score_train[3]))\\nprint(\\'train F1\\', f1_train)\\n\\nprint(\"*************************************\")\\nprint(\\'test loss: {}\\'.format(score[0]))\\nprint(\\'test acc: {}\\'.format(score[1]))\\nprint(\\'test precision: {}\\'.format(score[2]))\\nprint(\\'test recall: {}\\'.format(score[3]))\\nprint(\\'test F1\\', f1_test)\\n\\nprint(\"*************************************\")\\nweights = model.layers[0].get_weights()[0]\\nbiases = model.layers[0].get_weights()[1]\\nprint(\\'test weights \\',weights)\\nprint(\\'test biases \\',biases)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. regression logistic + 2 layers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b6mNzKraF6HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.python.keras.backend import shape\n",
        "model =keras.Sequential([\n",
        "                         keras.Input(shape=input_shape),\n",
        "                         layers.Dense(200, activation='relu'),\n",
        "                         layers.Dense(200, activation='relu'),\n",
        "                         layers.Flatten(),\n",
        "                         layers.Dense(num_classes, activation=tf.nn.softmax)\n",
        "\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "batch_size = 100\n",
        "epochs = 12\n",
        "metrics = ['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "PTVMQ9zhabnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e559ddf-ed87-447e-f11b-6ac04508271a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 28, 28, 200)       400       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 28, 28, 200)       40200     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 156800)            0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1568010   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,608,610\n",
            "Trainable params: 1,608,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run model and results"
      ],
      "metadata": {
        "id": "0ZqQGE-DLHRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JY-RFvANLDLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_split = .1) \n",
        "print(\"*************************************\")\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_train  = model.evaluate(X_train, y_train)\n",
        "f1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\n",
        "f1_test = 2*(score[2]*score[3])/(score[2]+score[3])\n",
        "\n",
        "print(\"Summary Model \")\n",
        "print('train loss: {}'.format(score_train[0]))\n",
        "print('train acc: {}'.format(score_train[1]))\n",
        "print('train precision: {}'.format(score_train[2]))\n",
        "print('train recall: {}'.format(score_train[3]))\n",
        "print('train F1', f1_train)\n",
        "\n",
        "print(\"*************************************\")\n",
        "print('test loss: {}'.format(score[0]))\n",
        "print('test acc: {}'.format(score[1]))\n",
        "print('test precision: {}'.format(score[2]))\n",
        "print('test recall: {}'.format(score[3]))\n",
        "print('test F1', f1_test)\n",
        "\n",
        "print(\"*************************************\")\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "biases = model.layers[0].get_weights()[1]\n",
        "print('test weights ',weights)\n",
        "print('test biases ',biases)"
      ],
      "metadata": {
        "id": "hASuW5OJabry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7698fcfd-5b90-4d51-8a72-8c88a93cf177"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "540/540 [==============================] - 27s 48ms/step - loss: 0.3631 - acc: 0.8958 - precision_4: 0.9217 - recall_4: 0.8699 - val_loss: 0.2447 - val_acc: 0.9315 - val_precision_4: 0.9448 - val_recall_4: 0.9178\n",
            "Epoch 2/12\n",
            "540/540 [==============================] - 25s 46ms/step - loss: 0.2913 - acc: 0.9184 - precision_4: 0.9341 - recall_4: 0.9041 - val_loss: 0.2305 - val_acc: 0.9345 - val_precision_4: 0.9466 - val_recall_4: 0.9248\n",
            "Epoch 3/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.2760 - acc: 0.9226 - precision_4: 0.9380 - recall_4: 0.9093 - val_loss: 0.2237 - val_acc: 0.9400 - val_precision_4: 0.9513 - val_recall_4: 0.9308\n",
            "Epoch 4/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.2584 - acc: 0.9281 - precision_4: 0.9411 - recall_4: 0.9164 - val_loss: 0.2131 - val_acc: 0.9412 - val_precision_4: 0.9531 - val_recall_4: 0.9323\n",
            "Epoch 5/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.2351 - acc: 0.9329 - precision_4: 0.9458 - recall_4: 0.9229 - val_loss: 0.2221 - val_acc: 0.9380 - val_precision_4: 0.9451 - val_recall_4: 0.9327\n",
            "Epoch 6/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.2179 - acc: 0.9379 - precision_4: 0.9496 - recall_4: 0.9283 - val_loss: 0.2093 - val_acc: 0.9403 - val_precision_4: 0.9541 - val_recall_4: 0.9292\n",
            "Epoch 7/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.2035 - acc: 0.9412 - precision_4: 0.9515 - recall_4: 0.9322 - val_loss: 0.2014 - val_acc: 0.9435 - val_precision_4: 0.9523 - val_recall_4: 0.9377\n",
            "Epoch 8/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.1951 - acc: 0.9432 - precision_4: 0.9532 - recall_4: 0.9349 - val_loss: 0.2085 - val_acc: 0.9402 - val_precision_4: 0.9507 - val_recall_4: 0.9323\n",
            "Epoch 9/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.1851 - acc: 0.9459 - precision_4: 0.9547 - recall_4: 0.9388 - val_loss: 0.2148 - val_acc: 0.9378 - val_precision_4: 0.9468 - val_recall_4: 0.9313\n",
            "Epoch 10/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.1795 - acc: 0.9473 - precision_4: 0.9563 - recall_4: 0.9397 - val_loss: 0.2101 - val_acc: 0.9427 - val_precision_4: 0.9503 - val_recall_4: 0.9378\n",
            "Epoch 11/12\n",
            "540/540 [==============================] - 25s 47ms/step - loss: 0.1751 - acc: 0.9478 - precision_4: 0.9574 - recall_4: 0.9413 - val_loss: 0.2200 - val_acc: 0.9435 - val_precision_4: 0.9484 - val_recall_4: 0.9398\n",
            "Epoch 12/12\n",
            "540/540 [==============================] - 25s 46ms/step - loss: 0.1721 - acc: 0.9496 - precision_4: 0.9582 - recall_4: 0.9423 - val_loss: 0.2109 - val_acc: 0.9433 - val_precision_4: 0.9504 - val_recall_4: 0.9380\n",
            "*************************************\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.2481 - acc: 0.9343 - precision_4: 0.9418 - recall_4: 0.9281\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1448 - acc: 0.9597 - precision_4: 0.9674 - recall_4: 0.9530\n",
            "Summary Model \n",
            "train loss: 0.144782155752182\n",
            "train acc: 0.9597166776657104\n",
            "train precision: 0.9674155712127686\n",
            "train recall: 0.9530333280563354\n",
            "train F1 0.9601705953199984\n",
            "*************************************\n",
            "test loss: 0.24807678163051605\n",
            "test acc: 0.9343000054359436\n",
            "test precision: 0.941755473613739\n",
            "test recall: 0.9280999898910522\n",
            "test F1 0.9348778690118416\n",
            "*************************************\n",
            "test weights  [[ 3.05423290e-02 -6.42102286e-02  4.96777426e-03  3.34039107e-02\n",
            "  -7.59497061e-02  1.84798297e-02  7.72281140e-02 -1.67607829e-01\n",
            "  -1.40846983e-01  1.69949904e-02  2.19484359e-01  6.49736747e-02\n",
            "   2.43462622e-02 -9.75661874e-02  1.77580848e-01 -7.33798742e-02\n",
            "  -1.07038148e-01 -9.11874101e-02  2.73090582e-02  3.17470953e-02\n",
            "  -1.48978740e-01  7.61171728e-02 -1.15153350e-01 -1.24441266e-01\n",
            "  -1.98558420e-02  1.12650894e-01  1.10030752e-02 -1.05186462e-01\n",
            "  -6.81647435e-02  2.70014051e-02  1.84830830e-01  1.01757310e-01\n",
            "  -3.32732052e-02  2.05023751e-01  2.40700096e-01  5.31700579e-03\n",
            "  -1.48325101e-01 -1.55750081e-01  1.81614056e-01 -6.73158467e-02\n",
            "  -9.95551646e-02 -1.08785823e-01 -1.31482571e-01 -1.22944817e-01\n",
            "  -1.63448289e-01 -9.81406197e-02  8.11996087e-02 -1.92076266e-02\n",
            "   3.43769379e-02  8.21112990e-02  3.12976055e-02  1.30474389e-01\n",
            "   9.19597521e-02 -1.03942603e-02 -6.91892803e-02  7.29714707e-02\n",
            "  -1.80897571e-03  1.38490707e-01 -1.63160607e-01 -2.82951742e-02\n",
            "  -1.39137447e-01 -5.53842857e-02  1.80689692e-01 -1.14391446e-02\n",
            "  -1.64305419e-01  3.61621566e-02 -7.12333620e-03 -4.37916815e-03\n",
            "   1.35524809e-01  7.73250358e-03 -1.60344154e-01  6.78323135e-02\n",
            "   1.26762748e-01 -1.71515837e-01  1.92335293e-01 -7.83962086e-02\n",
            "   1.35545522e-01  5.54442778e-03  1.46113053e-01 -1.47753268e-01\n",
            "   6.77101687e-02  1.46762520e-01  1.57659680e-01 -4.30120081e-02\n",
            "  -6.16799369e-02 -1.63331971e-01 -1.23456180e-01 -1.51229829e-01\n",
            "   1.83684394e-01  1.01543320e-02 -1.52591154e-01 -1.59537733e-01\n",
            "  -1.37054712e-01 -3.15096974e-03 -1.60505831e-01 -1.14399567e-01\n",
            "  -5.72917908e-02  6.71762452e-02  7.65918642e-02 -4.89250198e-02\n",
            "   7.07950369e-02 -9.26247835e-02 -3.95092964e-02  1.70677841e-01\n",
            "   1.55496404e-01  1.45379722e-01 -9.95464772e-02  5.70354760e-02\n",
            "   7.59683922e-02  1.36478581e-02 -1.31937340e-01 -3.06348354e-02\n",
            "   8.56371038e-03 -4.34064716e-02  9.28755105e-02 -4.75353897e-02\n",
            "   2.35279948e-01 -8.44384432e-02  2.09858902e-02 -4.02142256e-02\n",
            "  -6.56683594e-02  1.46323308e-01  9.24236551e-02 -2.76432838e-03\n",
            "  -9.20593739e-02  6.28723726e-02 -4.58842367e-02 -1.11136094e-01\n",
            "   4.37160507e-02  9.19875875e-02 -8.15004706e-02  1.06489457e-01\n",
            "   8.89702961e-02  1.96634382e-02  1.33670971e-01 -4.58259434e-02\n",
            "  -2.80484855e-02 -9.03458893e-02 -1.60710648e-01 -5.59962541e-03\n",
            "  -1.38772070e-01  1.66615427e-01  1.21306695e-01 -8.32049772e-02\n",
            "   1.71832919e-01  1.07097827e-01 -9.61938202e-02 -5.42363167e-01\n",
            "   2.20110506e-01  1.33474380e-01  1.63017765e-01 -6.38899133e-02\n",
            "  -3.89545560e-02  1.97278395e-01  1.45533115e-01  1.46113979e-02\n",
            "  -2.43029743e-02 -6.32001013e-02 -2.45736935e-03 -1.21179432e-01\n",
            "  -3.66420150e-02  1.44329920e-01 -1.70635104e-01  2.33154781e-02\n",
            "   1.74131304e-01 -9.84668732e-03 -1.60374552e-01 -1.71547890e-01\n",
            "  -8.47894400e-02  7.91997649e-03 -6.11415952e-02 -9.40148160e-03\n",
            "   2.10112125e-01 -8.46773610e-02 -1.50548220e-01 -9.04477239e-02\n",
            "   1.74873188e-01  2.39160150e-01  2.53914129e-02  4.30632047e-02\n",
            "  -6.66437969e-02  1.54250935e-01 -9.13046077e-02 -3.38602066e-02\n",
            "  -2.81359255e-02 -7.20648840e-02  1.80485412e-01  1.16595127e-01\n",
            "  -3.37790102e-02 -2.28766799e-02 -3.54588032e-04 -5.39146662e-02\n",
            "  -8.02076086e-02 -8.54160190e-02  2.80336253e-02 -1.69381693e-01\n",
            "   1.45395547e-01 -5.32533601e-02 -1.03729486e-01  2.51451880e-01]]\n",
            "test biases  [-3.12013514e-02  0.00000000e+00 -6.28768513e-03 -3.50020193e-02\n",
            "  0.00000000e+00 -2.11299192e-02 -3.85351107e-02  0.00000000e+00\n",
            "  0.00000000e+00 -1.95988677e-02 -1.67432777e-03 -4.25430462e-02\n",
            " -1.44693991e-02  0.00000000e+00 -2.45246268e-03  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -1.93783883e-02 -2.41832584e-02\n",
            "  0.00000000e+00 -1.25136282e-02  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -9.77973342e-02 -1.44303553e-02  0.00000000e+00\n",
            "  0.00000000e+00 -2.99100205e-02 -8.98521277e-04 -9.14319530e-02\n",
            "  0.00000000e+00 -1.36380956e-01 -2.15342874e-03 -9.14752297e-03\n",
            "  0.00000000e+00  0.00000000e+00 -6.90726284e-03  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -7.26474077e-02  0.00000000e+00\n",
            " -3.52840349e-02 -3.56388576e-02 -3.85714360e-02 -8.67015570e-02\n",
            " -4.67504598e-02  0.00000000e+00  0.00000000e+00  7.37968483e-04\n",
            " -1.07630587e-03 -3.41785513e-03  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -1.38209298e-01  0.00000000e+00\n",
            "  0.00000000e+00 -4.51495945e-02  0.00000000e+00  0.00000000e+00\n",
            " -1.11312188e-01 -1.22768329e-02  0.00000000e+00 -5.70289753e-02\n",
            " -1.03352718e-01  0.00000000e+00 -1.26691244e-04  0.00000000e+00\n",
            " -6.86812389e-04 -1.01193236e-02 -2.81457766e-03  0.00000000e+00\n",
            " -3.46457548e-02 -6.33920133e-02 -1.23942919e-01  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            " -3.90290865e-04 -1.38844140e-02  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -7.23056570e-02 -1.59121659e-02  0.00000000e+00\n",
            " -6.05323277e-02  0.00000000e+00  0.00000000e+00 -1.13677517e-01\n",
            " -1.03202939e-01 -1.69379101e-03  0.00000000e+00 -5.82221150e-02\n",
            " -6.33697137e-02 -1.48061644e-02  0.00000000e+00  0.00000000e+00\n",
            " -1.05602704e-02  0.00000000e+00 -8.09881911e-02  0.00000000e+00\n",
            " -1.54461443e-01  0.00000000e+00 -2.46414747e-02  0.00000000e+00\n",
            "  0.00000000e+00 -1.19197503e-01 -8.16677958e-02 -2.47964798e-03\n",
            "  0.00000000e+00 -6.65317997e-02  0.00000000e+00  0.00000000e+00\n",
            " -4.81693037e-02 -3.15459818e-03  0.00000000e+00 -8.72609764e-02\n",
            " -7.94716924e-02 -2.17268132e-02 -1.10620737e-01  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.08029740e-02\n",
            "  0.00000000e+00 -9.44162253e-04 -9.31249410e-02  0.00000000e+00\n",
            " -1.13335989e-01 -8.80650729e-02  0.00000000e+00  1.28695056e-01\n",
            " -1.44502550e-01 -6.96084350e-02 -2.68456177e-03  0.00000000e+00\n",
            "  0.00000000e+00 -1.05112384e-03 -1.13746785e-01 -1.92260239e-02\n",
            "  0.00000000e+00  0.00000000e+00 -1.39013324e-02  0.00000000e+00\n",
            "  0.00000000e+00 -1.13310441e-01  0.00000000e+00 -2.56170332e-02\n",
            " -2.36040913e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -1.13621457e-02  0.00000000e+00 -1.65264378e-03\n",
            " -1.15113356e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            " -6.91405439e-04  6.88858377e-03 -2.99717188e-02 -4.40983735e-02\n",
            "  0.00000000e+00 -7.19163790e-02  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -3.87831824e-04 -5.49552962e-02\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00 -2.94662379e-02  0.00000000e+00\n",
            " -9.64225605e-02  0.00000000e+00  0.00000000e+00  3.15735606e-03]\n",
            "CPU times: user 1min 38s, sys: 18.7 s, total: 1min 56s\n",
            "Wall time: 6min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. convolution\n"
      ],
      "metadata": {
        "id": "kBL540wC7vlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "                          keras.Input(shape=input_shape),\n",
        "                          layers.Conv2D(32, kernel_size=(3,3),padding='same',strides=1),\n",
        "                          layers.MaxPooling2D(pool_size=(2,2),strides=2),\n",
        "                          layers.Dense(1024, activation=tf.keras.activations.relu),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(num_classes, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "batch_size = 100\n",
        "epochs = 12\n",
        "metrics = ['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "LouSjCcJYfON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2534fdce-64d5-42f4-dfad-ffbdb8eab5ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 14, 14, 1024)      33792     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 200704)            0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                2007050   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,041,162\n",
            "Trainable params: 2,041,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run model and results"
      ],
      "metadata": {
        "id": "fcKFJflL_AE3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmByd46BR6iN",
        "outputId": "3fd7970e-4508-4832-eedb-007a32925737"
      },
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          # X_val, y_val)\n",
        "          validation_split = .1) #validation_date=(X_val, y_val)\n",
        "print(\"*************************************\")\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_train  = model.evaluate(X_train, y_train)\n",
        "f1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\n",
        "f1_test = 2*(score[2]*score[3])/(score[2]+score[3])\n",
        "\n",
        "print(\"Summary Model \")\n",
        "print('train loss: {}'.format(score_train[0]))\n",
        "print('train acc: {}'.format(score_train[1]))\n",
        "print('train precision: {}'.format(score_train[2]))\n",
        "print('train recall: {}'.format(score_train[3]))\n",
        "print('train F1', f1_train)\n",
        "\n",
        "print(\"*************************************\")\n",
        "print('test loss: {}'.format(score[0]))\n",
        "print('test acc: {}'.format(score[1]))\n",
        "print('test precision: {}'.format(score[2]))\n",
        "print('test recall: {}'.format(score[3]))\n",
        "print('test F1', f1_test)\n",
        "\n",
        "print(\"*************************************\")\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "biases = model.layers[0].get_weights()[1]\n",
        "print('test weights ',weights)\n",
        "print('test biases ',biases)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "540/540 [==============================] - 26s 34ms/step - loss: 0.1647 - acc: 0.9507 - precision_5: 0.9661 - recall_5: 0.9377 - val_loss: 0.0646 - val_acc: 0.9822 - val_precision_5: 0.9838 - val_recall_5: 0.9807\n",
            "Epoch 2/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0569 - acc: 0.9826 - precision_5: 0.9845 - recall_5: 0.9810 - val_loss: 0.0493 - val_acc: 0.9873 - val_precision_5: 0.9880 - val_recall_5: 0.9855\n",
            "Epoch 3/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0402 - acc: 0.9875 - precision_5: 0.9890 - recall_5: 0.9865 - val_loss: 0.0519 - val_acc: 0.9872 - val_precision_5: 0.9886 - val_recall_5: 0.9863\n",
            "Epoch 4/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0277 - acc: 0.9914 - precision_5: 0.9922 - recall_5: 0.9907 - val_loss: 0.0529 - val_acc: 0.9870 - val_precision_5: 0.9881 - val_recall_5: 0.9863\n",
            "Epoch 5/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0214 - acc: 0.9930 - precision_5: 0.9936 - recall_5: 0.9925 - val_loss: 0.0589 - val_acc: 0.9850 - val_precision_5: 0.9863 - val_recall_5: 0.9850\n",
            "Epoch 6/12\n",
            "540/540 [==============================] - 18s 33ms/step - loss: 0.0164 - acc: 0.9948 - precision_5: 0.9953 - recall_5: 0.9945 - val_loss: 0.0691 - val_acc: 0.9868 - val_precision_5: 0.9872 - val_recall_5: 0.9863\n",
            "Epoch 7/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0132 - acc: 0.9954 - precision_5: 0.9957 - recall_5: 0.9952 - val_loss: 0.0619 - val_acc: 0.9850 - val_precision_5: 0.9858 - val_recall_5: 0.9847\n",
            "Epoch 8/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0126 - acc: 0.9956 - precision_5: 0.9959 - recall_5: 0.9954 - val_loss: 0.0732 - val_acc: 0.9852 - val_precision_5: 0.9858 - val_recall_5: 0.9848\n",
            "Epoch 9/12\n",
            "540/540 [==============================] - 18s 34ms/step - loss: 0.0105 - acc: 0.9964 - precision_5: 0.9965 - recall_5: 0.9963 - val_loss: 0.0714 - val_acc: 0.9855 - val_precision_5: 0.9860 - val_recall_5: 0.9855\n",
            "Epoch 10/12\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.0062 - acc: 0.9978 - precision_5: 0.9979 - recall_5: 0.9977 - val_loss: 0.0799 - val_acc: 0.9860 - val_precision_5: 0.9862 - val_recall_5: 0.9858\n",
            "Epoch 11/12\n",
            "540/540 [==============================] - 18s 33ms/step - loss: 0.0052 - acc: 0.9982 - precision_5: 0.9983 - recall_5: 0.9982 - val_loss: 0.0868 - val_acc: 0.9865 - val_precision_5: 0.9867 - val_recall_5: 0.9865\n",
            "Epoch 12/12\n",
            "540/540 [==============================] - 18s 33ms/step - loss: 0.0066 - acc: 0.9977 - precision_5: 0.9978 - recall_5: 0.9976 - val_loss: 0.1036 - val_acc: 0.9802 - val_precision_5: 0.9803 - val_recall_5: 0.9802\n",
            "*************************************\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0808 - acc: 0.9827 - precision_5: 0.9834 - recall_5: 0.9826\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0192 - acc: 0.9949 - precision_5: 0.9951 - recall_5: 0.9949\n",
            "Summary Model \n",
            "train loss: 0.01920836605131626\n",
            "train acc: 0.994949996471405\n",
            "train precision: 0.9951323866844177\n",
            "train recall: 0.9949333071708679\n",
            "train F1 0.9950328369700187\n",
            "*************************************\n",
            "test loss: 0.08078764379024506\n",
            "test acc: 0.982699990272522\n",
            "test precision: 0.9833866953849792\n",
            "test recall: 0.9825999736785889\n",
            "test F1 0.9829931771220078\n",
            "*************************************\n",
            "test weights  [[[[ 0.19344367  0.10735274 -0.17040382 -0.28884697 -0.2530766\n",
            "     0.01334535 -0.14747879  0.13303766  0.07539545  0.03243016\n",
            "     0.13959381  0.12285832  0.10721888 -0.02583373 -0.13673761\n",
            "    -0.06393982 -0.12808113  0.17112532  0.07182724  0.17454922\n",
            "     0.09169223  0.08642361  0.06278072 -0.06057746 -0.2698151\n",
            "     0.02764119 -0.050293    0.16333073 -0.18065915 -0.08223902\n",
            "     0.24964644 -0.13039365]]\n",
            "\n",
            "  [[-0.10901328  0.14624049  0.02644716 -0.19355667 -0.19576894\n",
            "     0.15160285  0.04294594  0.09573717 -0.02499995 -0.04248844\n",
            "     0.14513628 -0.08368386  0.04375973 -0.12062444  0.09084053\n",
            "     0.19573002 -0.10338259 -0.22743425  0.06546983 -0.00261715\n",
            "     0.17913364  0.02439903  0.23250021  0.13184448 -0.2511295\n",
            "     0.18911351 -0.30994463  0.06868158 -0.23760389  0.01843211\n",
            "     0.1000662  -0.17265   ]]\n",
            "\n",
            "  [[-0.30634162 -0.1140691   0.13827702  0.10951747 -0.0079064\n",
            "    -0.00377631  0.05733987  0.15663731  0.0642494   0.3151301\n",
            "     0.03109025 -0.25309023 -0.07463694  0.08960623  0.04213339\n",
            "     0.09656446  0.0966654  -0.07235511 -0.2740156  -0.2229217\n",
            "     0.16211227  0.13479196 -0.11981336 -0.0642943  -0.23729947\n",
            "     0.14153421 -0.08605091 -0.21146141 -0.15464206  0.19117138\n",
            "    -0.1866506   0.0347061 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.18806118 -0.2189187  -0.05896559  0.13068444  0.07407131\n",
            "    -0.05711468 -0.27557486  0.17480391  0.0387507  -0.23697859\n",
            "    -0.17164671 -0.00376113  0.107391    0.08637843  0.03657165\n",
            "    -0.11020946 -0.2928595   0.22359021  0.13583878  0.13783143\n",
            "     0.17343609  0.18694259 -0.07972997  0.13871951  0.0885694\n",
            "    -0.26124194  0.1881713   0.04115768 -0.19747803 -0.23403333\n",
            "    -0.10013383  0.02200806]]\n",
            "\n",
            "  [[ 0.02912225 -0.09818351  0.1644245   0.07752588  0.177693\n",
            "     0.02513682  0.20255649 -0.0658994   0.20551959 -0.26792836\n",
            "     0.09822817  0.15152429 -0.0206485   0.13690975  0.15103067\n",
            "    -0.01920579  0.09032057 -0.14670244  0.10469107  0.02543202\n",
            "     0.00675391  0.12748629  0.16481993  0.20450489 -0.12332167\n",
            "    -0.14272535 -0.1085124  -0.3589431  -0.03189838 -0.13636512\n",
            "    -0.2717146  -0.14162551]]\n",
            "\n",
            "  [[-0.18917929  0.19393672 -0.2245247  -0.09711039  0.1290257\n",
            "     0.10838369  0.00680258 -0.03572452  0.15763225  0.16137177\n",
            "    -0.10293989  0.20109041 -0.00638631 -0.0943711  -0.12346891\n",
            "     0.14747727  0.07528768 -0.09824426 -0.25033122 -0.20205076\n",
            "    -0.09664149  0.00751773  0.04293059 -0.18754324 -0.09197639\n",
            "    -0.08375977 -0.1741093  -0.02499319  0.16191782 -0.23174207\n",
            "    -0.10233996 -0.16950512]]]\n",
            "\n",
            "\n",
            " [[[ 0.04079594 -0.27942613 -0.00733015  0.17958644  0.14857274\n",
            "    -0.25401485 -0.00561344 -0.13393034 -0.23986866  0.069278\n",
            "    -0.16767244 -0.21634555  0.12359677 -0.21503921 -0.06342366\n",
            "    -0.26285735 -0.24382259  0.02136835  0.03792323  0.20176044\n",
            "    -0.20026827 -0.01189747 -0.33912233  0.16408508  0.20902508\n",
            "     0.10318706  0.22033693 -0.11848903 -0.06024139  0.18505631\n",
            "    -0.12848085  0.11008527]]\n",
            "\n",
            "  [[ 0.10623883 -0.04049323  0.17245154  0.05396099 -0.16524029\n",
            "    -0.14800131  0.20790637 -0.10763764 -0.22004706 -0.13614663\n",
            "    -0.2109657  -0.00618879  0.04841902 -0.02092261  0.03934526\n",
            "    -0.23743546  0.07589756  0.11955313  0.07607843 -0.12875368\n",
            "    -0.21841225 -0.06676429 -0.074945   -0.17049786 -0.14868645\n",
            "    -0.12701178  0.1785783   0.02861463  0.20742261  0.03913243\n",
            "    -0.12641197  0.19106156]]\n",
            "\n",
            "  [[ 0.07070671  0.21417318 -0.08505931  0.03938822 -0.23181094\n",
            "     0.19052745 -0.21829438 -0.12694094 -0.21888128  0.04749247\n",
            "    -0.17764312 -0.00280612  0.06108062  0.14327693  0.05960837\n",
            "     0.1494684   0.07874142 -0.07490136  0.06506702 -0.19931787\n",
            "    -0.10438369 -0.19597372  0.01334308 -0.28156117 -0.30904767\n",
            "    -0.01581103  0.1733308   0.20521171  0.218984   -0.18623525\n",
            "     0.11828896  0.20104283]]]]\n",
            "test biases  [-0.04042815 -0.03571507 -0.035852   -0.01842591 -0.02084985 -0.04853268\n",
            " -0.02707778 -0.01724341 -0.00326409 -0.01929521 -0.00870912 -0.05054173\n",
            " -0.02661736 -0.02013913 -0.02746397 -0.02988458  0.0013885  -0.01404929\n",
            " -0.03962808 -0.01498969 -0.00456996 -0.01030031 -0.03359076 -0.00445094\n",
            "  0.01994804 -0.01550028 -0.0319836  -0.04343759  0.00736137 -0.00774518\n",
            "  0.02216946  0.00656499]\n",
            "CPU times: user 1min 31s, sys: 16 s, total: 1min 47s\n",
            "Wall time: 4min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. convolution add layer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qIoV0ba7DKiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "                          keras.Input(shape=input_shape),\n",
        "                          layers.Conv2D(32, kernel_size=(3,3),padding='same',strides=1),\n",
        "                          layers.Conv2D(64, kernel_size=(5,5),padding='same',strides=1),\n",
        "                          layers.MaxPooling2D(pool_size=(2,2),strides=2),\n",
        "                          layers.Dense(1024, activation=tf.keras.activations.relu),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(num_classes, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "batch_size = 100\n",
        "epochs = 12\n",
        "metrics = ['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7DdUyzkAfIY",
        "outputId": "6104eeda-00da-4421-dfd2-49ad7cd94720"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 64)        51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 14, 14, 1024)      66560     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 200704)            0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                2007050   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,125,194\n",
            "Trainable params: 2,125,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run model and results"
      ],
      "metadata": {
        "id": "FOeDNK8CLRq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_split = .1) \n",
        "\n",
        "\n",
        "print(\"*************************************\")\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_train  = model.evaluate(X_train, y_train)\n",
        "f1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\n",
        "f1_test = 2*(score[2]*score[3])/(score[2]+score[3])\n",
        "\n",
        "print(\"Summary Model \")\n",
        "print('train loss: {}'.format(score_train[0]))\n",
        "print('train acc: {}'.format(score_train[1]))\n",
        "print('train precision: {}'.format(score_train[2]))\n",
        "print('train recall: {}'.format(score_train[3]))\n",
        "print('train F1', f1_train)\n",
        "\n",
        "print(\"*************************************\")\n",
        "print('test loss: {}'.format(score[0]))\n",
        "print('test acc: {}'.format(score[1]))\n",
        "print('test precision: {}'.format(score[2]))\n",
        "print('test recall: {}'.format(score[3]))\n",
        "print('test F1', f1_test)\n",
        "\n",
        "print(\"*************************************\")\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "biases = model.layers[0].get_weights()[1]\n",
        "print('test weights ',weights)\n",
        "print('test biases ',biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3db6QZ_Lz50",
        "outputId": "2f9c0cbb-d517-44c5-f5df-5af9ad1628e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "540/540 [==============================] - 24s 42ms/step - loss: 0.1342 - acc: 0.9594 - precision_6: 0.9703 - recall_6: 0.9520 - val_loss: 0.0527 - val_acc: 0.9842 - val_precision_6: 0.9860 - val_recall_6: 0.9827\n",
            "Epoch 2/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0463 - acc: 0.9856 - precision_6: 0.9868 - recall_6: 0.9844 - val_loss: 0.0496 - val_acc: 0.9867 - val_precision_6: 0.9883 - val_recall_6: 0.9858\n",
            "Epoch 3/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0328 - acc: 0.9895 - precision_6: 0.9901 - recall_6: 0.9889 - val_loss: 0.0432 - val_acc: 0.9897 - val_precision_6: 0.9905 - val_recall_6: 0.9892\n",
            "Epoch 4/12\n",
            "540/540 [==============================] - 22s 41ms/step - loss: 0.0228 - acc: 0.9926 - precision_6: 0.9931 - recall_6: 0.9923 - val_loss: 0.0453 - val_acc: 0.9878 - val_precision_6: 0.9888 - val_recall_6: 0.9872\n",
            "Epoch 5/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0175 - acc: 0.9943 - precision_6: 0.9945 - recall_6: 0.9941 - val_loss: 0.0480 - val_acc: 0.9868 - val_precision_6: 0.9880 - val_recall_6: 0.9862\n",
            "Epoch 6/12\n",
            "540/540 [==============================] - 22s 41ms/step - loss: 0.0133 - acc: 0.9956 - precision_6: 0.9958 - recall_6: 0.9954 - val_loss: 0.0640 - val_acc: 0.9877 - val_precision_6: 0.9878 - val_recall_6: 0.9877\n",
            "Epoch 7/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0142 - acc: 0.9955 - precision_6: 0.9956 - recall_6: 0.9954 - val_loss: 0.0564 - val_acc: 0.9883 - val_precision_6: 0.9883 - val_recall_6: 0.9883\n",
            "Epoch 8/12\n",
            "540/540 [==============================] - 22s 41ms/step - loss: 0.0097 - acc: 0.9970 - precision_6: 0.9971 - recall_6: 0.9969 - val_loss: 0.0620 - val_acc: 0.9872 - val_precision_6: 0.9877 - val_recall_6: 0.9870\n",
            "Epoch 9/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0103 - acc: 0.9967 - precision_6: 0.9968 - recall_6: 0.9966 - val_loss: 0.0700 - val_acc: 0.9870 - val_precision_6: 0.9872 - val_recall_6: 0.9867\n",
            "Epoch 10/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0093 - acc: 0.9970 - precision_6: 0.9971 - recall_6: 0.9970 - val_loss: 0.0539 - val_acc: 0.9903 - val_precision_6: 0.9905 - val_recall_6: 0.9902\n",
            "Epoch 11/12\n",
            "540/540 [==============================] - 22s 40ms/step - loss: 0.0088 - acc: 0.9971 - precision_6: 0.9972 - recall_6: 0.9971 - val_loss: 0.0826 - val_acc: 0.9872 - val_precision_6: 0.9873 - val_recall_6: 0.9870\n",
            "Epoch 12/12\n",
            "540/540 [==============================] - 22s 41ms/step - loss: 0.0070 - acc: 0.9976 - precision_6: 0.9977 - recall_6: 0.9976 - val_loss: 0.0708 - val_acc: 0.9883 - val_precision_6: 0.9887 - val_recall_6: 0.9883\n",
            "*************************************\n",
            "313/313 [==============================] - 4s 10ms/step - loss: 0.0659 - acc: 0.9872 - precision_6: 0.9873 - recall_6: 0.9871\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0113 - acc: 0.9973 - precision_6: 0.9973 - recall_6: 0.9972\n",
            "Summary Model \n",
            "train loss: 0.011287452653050423\n",
            "train acc: 0.9972500205039978\n",
            "train precision: 0.9973496198654175\n",
            "train recall: 0.9972166419029236\n",
            "train F1 0.9972831264513425\n",
            "*************************************\n",
            "test loss: 0.0658736303448677\n",
            "test acc: 0.9872000217437744\n",
            "test precision: 0.9872974753379822\n",
            "test recall: 0.9871000051498413\n",
            "test F1 0.9871987303688801\n",
            "*************************************\n",
            "test weights  [[[[-1.67485073e-01 -3.59892622e-02  4.14634719e-02 -2.00738311e-01\n",
            "     8.72798078e-03  3.01510841e-03 -5.49552143e-02  1.10733345e-01\n",
            "     3.28682177e-03  1.32650450e-01 -5.49111962e-02 -7.36729875e-02\n",
            "     3.29594053e-02  3.15626115e-02  9.09448639e-02 -7.74564892e-02\n",
            "     1.42376095e-01  1.34364977e-01 -1.04237132e-01  3.11869513e-02\n",
            "     3.78635116e-02  3.72266620e-02  7.34308688e-03 -1.46223634e-01\n",
            "    -6.62880465e-02  3.26363668e-02  1.44690713e-02 -4.31786338e-03\n",
            "    -2.34791059e-02 -6.22945987e-02  5.17237186e-02 -6.87820166e-02]]\n",
            "\n",
            "  [[ 1.24605224e-02  1.10835142e-01 -2.42374968e-02  1.32854015e-01\n",
            "    -2.14000866e-02  8.82029533e-02 -1.13898888e-01  1.34067103e-01\n",
            "     2.73489486e-02 -1.19789947e-04 -8.91392604e-02  2.18744054e-02\n",
            "    -9.76745272e-04 -1.81028340e-02 -7.73599464e-03  4.26538661e-02\n",
            "     1.28855333e-01  8.15634355e-02 -9.64505151e-02 -1.33478669e-02\n",
            "    -5.14250360e-02 -5.19336425e-02  3.72953936e-02  1.52282059e-01\n",
            "     2.29746792e-02  8.58773850e-03  3.98731884e-03 -3.59899029e-02\n",
            "     6.71038777e-02 -4.84112324e-03  1.61757588e-01 -1.11005539e-02]]\n",
            "\n",
            "  [[ 1.01456963e-01 -1.10482343e-03  1.05636992e-01 -1.61524757e-03\n",
            "     6.45444319e-02 -5.07408269e-02  1.68902621e-01  3.90337892e-02\n",
            "    -4.10919264e-02 -1.92734689e-01  1.15209214e-01  2.21483801e-02\n",
            "    -3.69021818e-02  5.11176512e-02 -4.19981731e-03 -4.39445823e-02\n",
            "     1.67301055e-02 -8.12880322e-02  1.73904989e-02  1.82884559e-02\n",
            "    -6.50037406e-03 -7.44651854e-02  6.02160580e-03  2.82890368e-02\n",
            "    -1.96188074e-02 -3.24643329e-02  4.97086756e-02  1.32309915e-02\n",
            "     3.97735015e-02  3.71952727e-02 -1.81293845e-01  2.39876332e-04]]]\n",
            "\n",
            "\n",
            " [[[-6.90878928e-02 -3.08116376e-02 -6.65568858e-02  1.20514527e-01\n",
            "    -3.00155785e-02 -1.80167705e-02  8.35103467e-02  8.83013085e-02\n",
            "    -2.68913675e-02  9.94352475e-02 -1.45787194e-01  7.04369843e-02\n",
            "    -4.05460224e-02 -7.92534724e-02  1.18349209e-01  1.16144612e-01\n",
            "    -9.15534422e-02 -1.60825208e-01  1.30407065e-01  7.39558646e-03\n",
            "     3.81464809e-02 -2.78784074e-02 -7.78093711e-02  9.88900363e-02\n",
            "     7.41208643e-02 -4.57546823e-02  3.29376906e-02  7.09581599e-02\n",
            "     1.12597041e-01  7.27062598e-02  1.94870383e-02  5.77371418e-02]]\n",
            "\n",
            "  [[ 1.38816312e-01 -9.41080377e-02  9.95943230e-03 -8.11924711e-02\n",
            "    -3.08836233e-02 -9.70944539e-02 -1.83522236e-02  6.77128062e-02\n",
            "     7.90928677e-02 -6.60044402e-02 -5.64941838e-02  4.63711917e-02\n",
            "     2.25714664e-03  1.79966763e-02  1.19977191e-01  3.52788940e-02\n",
            "    -1.63725629e-01 -2.47731805e-02  8.16419870e-02  5.23942038e-02\n",
            "    -1.79295167e-02  1.67714998e-01  9.73999575e-02 -1.71579525e-01\n",
            "     4.30057850e-03  9.42209139e-02  6.44863471e-02  1.05495997e-01\n",
            "    -3.07472982e-02 -2.50202790e-02  2.22306438e-02  5.69261089e-02]]\n",
            "\n",
            "  [[ 9.35262814e-02 -2.92166285e-02  1.59750879e-01  1.24467701e-01\n",
            "    -4.92260419e-03  2.93761808e-02 -8.47393721e-02 -3.90959084e-02\n",
            "    -8.05484597e-03  9.75464582e-02  1.30243421e-01 -1.03581607e-01\n",
            "     3.67071554e-02 -5.36046885e-02  6.62535727e-02  6.38288110e-02\n",
            "     1.07745253e-01 -1.18035965e-01 -3.33925039e-02 -9.13987234e-02\n",
            "     2.99768499e-03  1.28156841e-01 -2.93142572e-02  3.52396220e-02\n",
            "     7.62403086e-02  1.89024936e-02 -8.51740837e-02 -7.96773955e-02\n",
            "    -9.69409421e-02  2.42414046e-02 -1.35235816e-01 -1.83436833e-02]]]\n",
            "\n",
            "\n",
            " [[[ 1.10326074e-01  8.46635252e-02 -1.32517263e-01 -5.53692020e-02\n",
            "     4.30516377e-02  1.18508069e-02  4.70713293e-03 -6.11013500e-03\n",
            "     5.82302571e-04 -1.66648209e-01  1.57298610e-01  2.19876654e-02\n",
            "     2.27839071e-02  3.58637944e-02  9.95310619e-02 -1.37348950e-01\n",
            "    -3.31926756e-02  3.53130549e-02  6.15405813e-02 -3.35314944e-02\n",
            "    -9.54710972e-03 -4.53978740e-02  6.16889670e-02  1.56548977e-01\n",
            "    -6.98999017e-02 -9.97262672e-02 -1.25832841e-01 -1.12814970e-01\n",
            "    -1.06697962e-01  3.54236700e-02 -1.53535947e-01  1.02221109e-01]]\n",
            "\n",
            "  [[-9.01575312e-02  5.35148866e-02 -1.28134668e-01  7.49708265e-02\n",
            "     7.82344416e-02  2.88148075e-02  3.96518558e-02 -1.24369189e-01\n",
            "    -6.97135180e-02  1.03321545e-01 -9.21865776e-02  1.21740215e-01\n",
            "    -4.06600237e-02  1.38169155e-02  3.45150642e-02  5.48315123e-02\n",
            "    -9.52281803e-02  1.27288446e-01  3.70502844e-03 -2.55281609e-02\n",
            "     3.99546772e-02 -1.62989825e-01 -8.81102011e-02 -1.08928949e-01\n",
            "     2.33184788e-02  8.22749138e-02  3.71894874e-02 -6.50990335e-03\n",
            "     1.36148915e-01 -7.14731812e-02  9.38538695e-04 -1.17687479e-01]]\n",
            "\n",
            "  [[-8.63859355e-02 -3.09860017e-02 -5.27182668e-02 -1.59994915e-01\n",
            "    -6.61258698e-02  1.61795095e-02 -2.91321725e-02 -7.56686106e-02\n",
            "     2.88956314e-02  3.64009552e-02  7.14736357e-02 -1.12276770e-01\n",
            "     2.32226192e-03  1.03115644e-02  9.67533067e-02 -9.91721824e-02\n",
            "    -4.15368900e-02  4.89929505e-02 -9.89817083e-02  6.79772422e-02\n",
            "     4.60914448e-02 -2.67628040e-02  3.73824090e-02  2.82874033e-02\n",
            "    -8.97843093e-02 -9.49133560e-02 -1.34793492e-02  2.94500533e-02\n",
            "    -1.13252275e-01  9.03202221e-03  2.21543938e-01  2.40885410e-02]]]]\n",
            "test biases  [-0.00543615 -0.00448235  0.00209677  0.00600198 -0.00411811 -0.00011753\n",
            "  0.00266769 -0.00878171 -0.00018802 -0.00800038 -0.00753409  0.00211663\n",
            "  0.00346806 -0.00155916 -0.02576913  0.00484107 -0.00619797 -0.00825464\n",
            "  0.01609155 -0.00083551 -0.01161376  0.00638935 -0.00481798 -0.00068237\n",
            "  0.00222036  0.00127301 -0.00075621  0.00154278  0.0023115  -0.00340047\n",
            " -0.00327119  0.00241276]\n",
            "CPU times: user 1min 41s, sys: 16.5 s, total: 1min 58s\n",
            "Wall time: 5min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. convolution with Dropout\n"
      ],
      "metadata": {
        "id": "kn65lyACRuwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_nn_ops import relu\n",
        "from re import M\n",
        "from tensorflow.python.keras.layers.core import Dropout\n",
        "from tensorflow.python.ops.nn_ops import dropout\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.Input(shape=input_shape),\n",
        "                          layers.Conv2D(32, kernel_size=(3,3),padding='same',strides=1),\n",
        "                          layers.Conv2D(32, kernel_size=(5,5),padding='same',strides=1),\n",
        "                          layers.MaxPooling2D(pool_size=(2,2),strides=2),\n",
        "                          layers.Dense(1024, activation=tf.keras.activations.relu),\n",
        "                          layers.Dropout(.5),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(num_classes, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "batch_size = 100\n",
        "epochs = 12\n",
        "metrics = ['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPUUPLtHAgIa",
        "outputId": "f5917fb3-4eca-4d9b-bd9d-b85626029c2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 14, 14, 1024)      33792     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 1024)      0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 200704)            0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                2007050   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,066,794\n",
            "Trainable params: 2,066,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run model and results"
      ],
      "metadata": {
        "id": "gatGQGYELV8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_split = .1) \n",
        "\n",
        "\n",
        "print(\"*************************************\")\n",
        "score = model.evaluate(X_test, y_test)\n",
        "score_train  = model.evaluate(X_train, y_train)\n",
        "f1_train  = 2*(score_train[2]*score_train[3])/(score_train[2]+score_train[3])\n",
        "f1_test = 2*(score[2]*score[3])/(score[2]+score[3])\n",
        "\n",
        "print(\"Summary Model \")\n",
        "print('train loss: {}'.format(score_train[0]))\n",
        "print('train acc: {}'.format(score_train[1]))\n",
        "print('train precision: {}'.format(score_train[2]))\n",
        "print('train recall: {}'.format(score_train[3]))\n",
        "print('train F1', f1_train)\n",
        "\n",
        "print(\"*************************************\")\n",
        "print('test loss: {}'.format(score[0]))\n",
        "print('test acc: {}'.format(score[1]))\n",
        "print('test precision: {}'.format(score[2]))\n",
        "print('test recall: {}'.format(score[3]))\n",
        "print('test F1', f1_test)\n",
        "\n",
        "print(\"*************************************\")\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "biases = model.layers[0].get_weights()[1]\n",
        "print('test weights ',weights)\n",
        "print('test biases ',biases)"
      ],
      "metadata": {
        "id": "4gtcHcyRAgMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff24db50-ac81-42f3-fde1-b63f6466891b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "540/540 [==============================] - 26s 46ms/step - loss: 0.1425 - acc: 0.9579 - precision_7: 0.9692 - recall_7: 0.9486 - val_loss: 0.0565 - val_acc: 0.9842 - val_precision_7: 0.9873 - val_recall_7: 0.9823\n",
            "Epoch 2/12\n",
            "540/540 [==============================] - 24s 44ms/step - loss: 0.0535 - acc: 0.9837 - precision_7: 0.9852 - recall_7: 0.9826 - val_loss: 0.0546 - val_acc: 0.9868 - val_precision_7: 0.9878 - val_recall_7: 0.9858\n",
            "Epoch 3/12\n",
            "540/540 [==============================] - 24s 45ms/step - loss: 0.0393 - acc: 0.9874 - precision_7: 0.9884 - recall_7: 0.9866 - val_loss: 0.0379 - val_acc: 0.9895 - val_precision_7: 0.9906 - val_recall_7: 0.9887\n",
            "Epoch 4/12\n",
            "540/540 [==============================] - 24s 44ms/step - loss: 0.0279 - acc: 0.9910 - precision_7: 0.9916 - recall_7: 0.9904 - val_loss: 0.0503 - val_acc: 0.9862 - val_precision_7: 0.9873 - val_recall_7: 0.9858\n",
            "Epoch 5/12\n",
            "540/540 [==============================] - 25s 45ms/step - loss: 0.0263 - acc: 0.9918 - precision_7: 0.9920 - recall_7: 0.9913 - val_loss: 0.0469 - val_acc: 0.9892 - val_precision_7: 0.9895 - val_recall_7: 0.9885\n",
            "Epoch 6/12\n",
            "540/540 [==============================] - 25s 46ms/step - loss: 0.0200 - acc: 0.9934 - precision_7: 0.9937 - recall_7: 0.9932 - val_loss: 0.0443 - val_acc: 0.9897 - val_precision_7: 0.9898 - val_recall_7: 0.9893\n",
            "Epoch 7/12\n",
            "540/540 [==============================] - 24s 45ms/step - loss: 0.0180 - acc: 0.9938 - precision_7: 0.9941 - recall_7: 0.9936 - val_loss: 0.0485 - val_acc: 0.9907 - val_precision_7: 0.9908 - val_recall_7: 0.9905\n",
            "Epoch 8/12\n",
            "540/540 [==============================] - 24s 44ms/step - loss: 0.0164 - acc: 0.9946 - precision_7: 0.9949 - recall_7: 0.9946 - val_loss: 0.0509 - val_acc: 0.9888 - val_precision_7: 0.9897 - val_recall_7: 0.9887\n",
            "Epoch 9/12\n",
            "540/540 [==============================] - 25s 46ms/step - loss: 0.0154 - acc: 0.9947 - precision_7: 0.9948 - recall_7: 0.9946 - val_loss: 0.0509 - val_acc: 0.9873 - val_precision_7: 0.9880 - val_recall_7: 0.9872\n",
            "Epoch 10/12\n",
            "540/540 [==============================] - 24s 44ms/step - loss: 0.0137 - acc: 0.9957 - precision_7: 0.9958 - recall_7: 0.9955 - val_loss: 0.0583 - val_acc: 0.9885 - val_precision_7: 0.9892 - val_recall_7: 0.9885\n",
            "Epoch 11/12\n",
            "540/540 [==============================] - 24s 45ms/step - loss: 0.0138 - acc: 0.9954 - precision_7: 0.9955 - recall_7: 0.9954 - val_loss: 0.0659 - val_acc: 0.9867 - val_precision_7: 0.9873 - val_recall_7: 0.9865\n",
            "Epoch 12/12\n",
            "540/540 [==============================] - 24s 44ms/step - loss: 0.0118 - acc: 0.9959 - precision_7: 0.9959 - recall_7: 0.9959 - val_loss: 0.0571 - val_acc: 0.9897 - val_precision_7: 0.9898 - val_recall_7: 0.9897\n",
            "*************************************\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0530 - acc: 0.9874 - precision_7: 0.9881 - recall_7: 0.9872\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0109 - acc: 0.9972 - precision_7: 0.9972 - recall_7: 0.9972\n",
            "Summary Model \n",
            "train loss: 0.010930308140814304\n",
            "train acc: 0.9971666932106018\n",
            "train precision: 0.9972331523895264\n",
            "train recall: 0.9971666932106018\n",
            "train F1 0.997199921692758\n",
            "*************************************\n",
            "test loss: 0.0530221052467823\n",
            "test acc: 0.9873999953269958\n",
            "test precision: 0.9880892634391785\n",
            "test recall: 0.9872000217437744\n",
            "test F1 0.9876444424307206\n",
            "*************************************\n",
            "test weights  [[[[ 0.02527457 -0.07295033  0.10389636 -0.11475844  0.08898848\n",
            "     0.04129931  0.01900934 -0.02768274  0.03968031 -0.05245348\n",
            "     0.03115977 -0.10543645 -0.06389971 -0.10222811 -0.0607724\n",
            "     0.00754029 -0.01990058 -0.13249004 -0.02485636 -0.09642436\n",
            "     0.01097921 -0.09207764 -0.04306737  0.0733508   0.10762588\n",
            "    -0.06183594  0.05583919  0.09623196 -0.01410722  0.07987156\n",
            "    -0.10353278  0.07921963]]\n",
            "\n",
            "  [[ 0.11800127  0.03298896 -0.07789255 -0.06020474  0.08021978\n",
            "     0.04442744 -0.02600154  0.00764296  0.11995777 -0.04912956\n",
            "    -0.09276249 -0.0726284  -0.05430527 -0.03127399 -0.03145223\n",
            "     0.01383707 -0.09196864  0.08997022  0.10273485  0.05331304\n",
            "    -0.04324199  0.08948629 -0.00788629  0.07869721 -0.08407941\n",
            "     0.07017972 -0.11953133  0.03659053  0.09371067 -0.13086846\n",
            "     0.05152822 -0.09442234]]\n",
            "\n",
            "  [[ 0.0010566   0.02417005 -0.02292093  0.05919463 -0.08275408\n",
            "    -0.07565968  0.107642   -0.11229633  0.15453593  0.10549047\n",
            "     0.03861289  0.12447713  0.02889544 -0.00268524  0.04401404\n",
            "     0.04388724 -0.05308628 -0.01745345 -0.08033435  0.0176461\n",
            "     0.03167598 -0.03185615 -0.06769388 -0.01492276  0.01688646\n",
            "     0.04655232  0.06371929 -0.15561603 -0.0723644   0.03715289\n",
            "    -0.15142229 -0.00398833]]]\n",
            "\n",
            "\n",
            " [[[-0.02543237 -0.03289203 -0.10163663  0.1169103  -0.0908162\n",
            "    -0.0835899  -0.09056266  0.15196007 -0.08863948 -0.069937\n",
            "    -0.09427793  0.05684434  0.14500922 -0.00589932  0.09015938\n",
            "     0.06636097  0.05929562 -0.02726061 -0.07649203  0.00281869\n",
            "    -0.03253117  0.11450399  0.14910807  0.06668991 -0.162594\n",
            "    -0.04633044 -0.01209262 -0.04847079  0.0101288   0.03559469\n",
            "    -0.0668603   0.10316583]]\n",
            "\n",
            "  [[-0.150996   -0.04552343  0.05825872  0.08718296  0.05379173\n",
            "     0.02061346 -0.05899015  0.12424152 -0.06830271 -0.05731061\n",
            "     0.14386411 -0.09127211 -0.05384744 -0.05725716 -0.0572691\n",
            "     0.00669599 -0.07379749  0.13734014 -0.08819894  0.10588423\n",
            "    -0.0082729  -0.0716731   0.11206301 -0.08284364  0.08615091\n",
            "     0.02859951  0.09778213  0.08796598 -0.09222917  0.00655564\n",
            "     0.03351575 -0.11706268]]\n",
            "\n",
            "  [[-0.06657197  0.00243399 -0.02675175 -0.06362291 -0.11926454\n",
            "     0.04330742 -0.10293913 -0.13848057 -0.03178034  0.069586\n",
            "    -0.04122772 -0.00898887  0.04493004 -0.11905361 -0.0188509\n",
            "    -0.02071946 -0.1380532   0.08376454  0.13081457 -0.0006948\n",
            "     0.0585364  -0.07177096  0.08706148 -0.04057864 -0.00206676\n",
            "    -0.10852826 -0.1159861   0.09773041 -0.10988947  0.03009364\n",
            "    -0.07141494  0.03976387]]]\n",
            "\n",
            "\n",
            " [[[ 0.01030797 -0.03388647 -0.05153571 -0.00555441 -0.01812395\n",
            "     0.08747171  0.08811624  0.09971338 -0.01921092  0.06450477\n",
            "     0.10605344  0.12361467 -0.02110899 -0.07271731 -0.02527705\n",
            "    -0.00127994  0.11494067 -0.01616495  0.08628514  0.11503676\n",
            "     0.03021503 -0.00647672 -0.08674985 -0.05489044  0.07549454\n",
            "     0.03610905 -0.00937378 -0.13226843 -0.05278672  0.07590996\n",
            "     0.09087451 -0.06503828]]\n",
            "\n",
            "  [[-0.0132119  -0.04273341  0.07736005 -0.01273714 -0.07829706\n",
            "    -0.0627246  -0.05373855  0.07201026 -0.0759539  -0.03704323\n",
            "    -0.15684503  0.10926457 -0.05610448 -0.11811006  0.01355374\n",
            "    -0.06169708  0.10004401 -0.0253591   0.01228475 -0.17555746\n",
            "    -0.05038502 -0.03678074 -0.14742704 -0.10106748 -0.00942113\n",
            "     0.0379703   0.03258337  0.08601014  0.13314277 -0.09489974\n",
            "     0.08432945  0.11721991]]\n",
            "\n",
            "  [[ 0.12686487  0.14077128  0.04750107  0.0086695   0.19147997\n",
            "     0.02871322  0.05977074 -0.11745402 -0.01554487  0.04406593\n",
            "     0.05229978 -0.17305665 -0.06928521 -0.0150387  -0.01628936\n",
            "    -0.02884278  0.10785913 -0.11562331 -0.10810646  0.04382089\n",
            "     0.00777368  0.11027548  0.01490398  0.08530156 -0.02891551\n",
            "    -0.04565038  0.00551968 -0.03399884 -0.11216374 -0.01521115\n",
            "     0.1068892  -0.00243437]]]]\n",
            "test biases  [-0.01912061 -0.00885384 -0.00913514  0.00116806 -0.01238887 -0.0090667\n",
            " -0.00570644  0.0105896  -0.00944861 -0.00827464 -0.00145661  0.00917407\n",
            "  0.01501173  0.02775986  0.00331854  0.01299446  0.00026504  0.01319774\n",
            "  0.00577401  0.00718241  0.00077239 -0.00634195  0.01676575  0.00255061\n",
            "  0.00442095  0.00219415 -0.0061093   0.00349591  0.01922968  0.00348083\n",
            "  0.00130931 -0.00850536]\n",
            "CPU times: user 1min 45s, sys: 17.7 s, total: 2min 2s\n",
            "Wall time: 6min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Visualization the  best convolution\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wadzQ_-ZyVjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def plot_digit(image, digit, plt, i):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "print(\"before model\")\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i in range(5):\n",
        "    image = random.choice(X_test).squeeze()\n",
        "    digit = np.argmax(image.reshape((1, 28, 28, 1))[0], axis=-1)\n",
        "    plot_digit(image, digit, plt, i)\n",
        "plt.show()\n",
        "\n",
        "print(\"after model\")\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i in range(5):\n",
        "    image = random.choice(X_test).squeeze()\n",
        "    digit = np.argmax(model.predict(image.reshape((1, 28, 28, 1)))[0], axis=-1)\n",
        "    plot_digit(image, digit, plt, i)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "n7Pk45kIyUPc",
        "outputId": "31acf324-8225-47f9-d779-366ef6ccfd75"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x720 with 5 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x720 with 5 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}